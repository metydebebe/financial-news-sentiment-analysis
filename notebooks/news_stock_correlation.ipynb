{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8194e3e3",
   "metadata": {},
   "source": [
    "# Correlation Between News and Stock Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4986a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98f4bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets from the specified folders\n",
    "def load_data():\n",
    "    stock_data_folder = '../data/yfinance_data/'  # Folder for stock data\n",
    "    analyst_ratings_file = '../data/raw_analyst_ratings.csv'  # Path for analyst ratings\n",
    "    \n",
    "    stock_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Load each historical data file into a dictionary\n",
    "        for file_name in os.listdir(stock_data_folder):\n",
    "            if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "                ticker = file_name.split('_')[0]  # Extract ticker symbol\n",
    "                stock_data[ticker] = pd.read_csv(os.path.join(stock_data_folder, file_name))\n",
    "        \n",
    "        # Load the analyst ratings (which includes news data)\n",
    "        news_df = pd.read_csv(analyst_ratings_file)\n",
    "        \n",
    "        return stock_data, news_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Load datasets\n",
    "stock_data, news_df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa91ce9",
   "metadata": {},
   "source": [
    "### Date Alignment\n",
    "\n",
    "Convert date columns in the news DataFrame to datetime format and normalize dates to ensure alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert the 'date' column in news_df to datetime\n",
    "    news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "    if news_df['date'].isnull().any():\n",
    "        print(\"Warning: Some dates could not be parsed and are set to NaT.\")\n",
    "\n",
    "    for ticker in stock_data:\n",
    "        stock_data[ticker]['Date'] = pd.to_datetime(stock_data[ticker]['Date'], errors='coerce')\n",
    "        stock_data[ticker] = stock_data[ticker].set_index('Date').resample('D').ffill().reset_index()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error converting date columns: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23fcd941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0                                           headline  \\\n",
      "10               10    30 Stocks Moving in Friday's Pre-Market Session   \n",
      "11               11  SVB Leerink Maintains Outperform on Agilent Te...   \n",
      "12               12  8 Stocks Moving In Thursday's After-Hours Session   \n",
      "13               13  Agilent Technologies shares are trading higher...   \n",
      "14               14  Agilent Technologies Q2 Adj. EPS $0.71 Beats $...   \n",
      "...             ...                                                ...   \n",
      "1407323     1413844             Top Narrow Based Indexes For August 29   \n",
      "1407324     1413845  Recap: Wednesday's Top Percentage Gainers and ...   \n",
      "1407325     1413846  UPDATE: Oppenheimer Color on China Zenix Auto ...   \n",
      "1407326     1413847  Oppenheimer Initiates China Zenix At Outperfor...   \n",
      "1407327     1413848  China Zenix Auto International Opens For Tradi...   \n",
      "\n",
      "                                                       url  \\\n",
      "10       https://www.benzinga.com/news/20/05/16092879/3...   \n",
      "11       https://www.benzinga.com/news/20/05/16092270/s...   \n",
      "12       https://www.benzinga.com/news/20/05/16089803/8...   \n",
      "13       https://www.benzinga.com/wiim/20/05/16089218/a...   \n",
      "14       https://www.benzinga.com/news/earnings/20/05/1...   \n",
      "...                                                    ...   \n",
      "1407323  https://www.benzinga.com/news/11/08/1888782/to...   \n",
      "1407324  https://www.benzinga.com/news/earnings/11/06/1...   \n",
      "1407325  https://www.benzinga.com/analyst-ratings/analy...   \n",
      "1407326  https://www.benzinga.com/analyst-ratings/price...   \n",
      "1407327  https://www.benzinga.com/news/ipos/11/05/10789...   \n",
      "\n",
      "                       publisher date stock  \n",
      "10                    Lisa Levin  NaT     A  \n",
      "11       vishwanath@benzinga.com  NaT     A  \n",
      "12                  Tyree Gorges  NaT     A  \n",
      "13             Benzinga Newsdesk  NaT     A  \n",
      "14             Benzinga Newsdesk  NaT     A  \n",
      "...                          ...  ...   ...  \n",
      "1407323            Monica Gerson  NaT    ZX  \n",
      "1407324             Benjamin Lee  NaT    ZX  \n",
      "1407325           BenzingaStaffL  NaT    ZX  \n",
      "1407326                Joe Young  NaT    ZX  \n",
      "1407327            Allie Wickman  NaT    ZX  \n",
      "\n",
      "[1351341 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(news_df[news_df['date'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93dc4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'date' could not be parsed\n",
    "news_df = news_df.dropna(subset=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "276a341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "705a07f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2020-06-05 10:30:54-04:00\n",
      "1      2020-06-03 10:45:20-04:00\n",
      "2      2020-05-26 04:30:07-04:00\n",
      "3      2020-05-22 12:45:06-04:00\n",
      "4      2020-05-22 11:38:59-04:00\n",
      "5      2020-05-22 11:23:25-04:00\n",
      "6      2020-05-22 09:36:20-04:00\n",
      "7      2020-05-22 09:07:04-04:00\n",
      "8      2020-05-22 08:37:59-04:00\n",
      "9      2020-05-22 08:06:17-04:00\n",
      "1433   2020-06-09 10:52:15-04:00\n",
      "1434   2020-06-08 11:29:29-04:00\n",
      "1435   2020-06-08 10:32:42-04:00\n",
      "1436   2020-06-05 07:40:08-04:00\n",
      "1437   2020-06-04 14:46:13-04:00\n",
      "1438   2020-06-03 10:19:06-04:00\n",
      "1439   2020-05-27 10:10:20-04:00\n",
      "1440   2020-05-27 07:32:46-04:00\n",
      "1441   2020-05-26 07:47:48-04:00\n",
      "1442   2020-05-18 10:50:11-04:00\n",
      "Name: date, dtype: datetime64[ns, UTC-04:00]\n"
     ]
    }
   ],
   "source": [
    "print(news_df['date'].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b4476f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for ticker in stock_data:\n",
    "        stock_data[ticker]['Date'] = pd.to_datetime(stock_data[ticker]['Date'], errors='coerce')\n",
    "        stock_data[ticker] = stock_data[ticker].dropna(subset=['Date'])\n",
    "        stock_data[ticker] = stock_data[ticker].set_index('Date').resample('D').ffill().reset_index()\n",
    "except Exception as e:\n",
    "    print(f\"Error converting date columns: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9573e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in AAPL before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for AAPL:\n",
      "0   1980-12-12\n",
      "1   1980-12-13\n",
      "2   1980-12-14\n",
      "3   1980-12-15\n",
      "4   1980-12-16\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for AAPL:\n",
      "0   1980-12-12\n",
      "1   1980-12-13\n",
      "2   1980-12-14\n",
      "3   1980-12-15\n",
      "4   1980-12-16\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "AAPL has 0 invalid dates (NaT) after conversion.\n",
      "AAPL shape after dropping NaT: (15937, 9)\n",
      "\n",
      "Sample data for AAPL after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
      "0 1980-12-12  0.128348  0.128906  0.128348  0.128348   0.098943  469033600   \n",
      "1 1980-12-13  0.128348  0.128906  0.128348  0.128348   0.098943  469033600   \n",
      "2 1980-12-14  0.128348  0.128906  0.128348  0.128348   0.098943  469033600   \n",
      "3 1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781  175884800   \n",
      "4 1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898  105728000   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in AMZN before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for AMZN:\n",
      "0   1997-05-15\n",
      "1   1997-05-16\n",
      "2   1997-05-17\n",
      "3   1997-05-18\n",
      "4   1997-05-19\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for AMZN:\n",
      "0   1997-05-15\n",
      "1   1997-05-16\n",
      "2   1997-05-17\n",
      "3   1997-05-18\n",
      "4   1997-05-19\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "AMZN has 0 invalid dates (NaT) after conversion.\n",
      "AMZN shape after dropping NaT: (9939, 9)\n",
      "\n",
      "Sample data for AMZN after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close      Volume  \\\n",
      "0 1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000   \n",
      "1 1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "2 1997-05-17  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "3 1997-05-18  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "4 1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in GOOG before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for GOOG:\n",
      "0   2004-08-19\n",
      "1   2004-08-20\n",
      "2   2004-08-21\n",
      "3   2004-08-22\n",
      "4   2004-08-23\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for GOOG:\n",
      "0   2004-08-19\n",
      "1   2004-08-20\n",
      "2   2004-08-21\n",
      "3   2004-08-22\n",
      "4   2004-08-23\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "GOOG has 0 invalid dates (NaT) after conversion.\n",
      "GOOG shape after dropping NaT: (7286, 9)\n",
      "\n",
      "Sample data for GOOG after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
      "0 2004-08-19  2.490664  2.591785  2.390042  2.499133   2.496292  897427216   \n",
      "1 2004-08-20  2.515820  2.716817  2.503118  2.697639   2.694573  458857488   \n",
      "2 2004-08-21  2.515820  2.716817  2.503118  2.697639   2.694573  458857488   \n",
      "3 2004-08-22  2.515820  2.716817  2.503118  2.697639   2.694573  458857488   \n",
      "4 2004-08-23  2.758411  2.826406  2.716070  2.724787   2.721690  366857939   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in META before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for META:\n",
      "0   2012-12-12\n",
      "1   2012-12-13\n",
      "2   2012-12-14\n",
      "3   2012-12-15\n",
      "4   2012-12-16\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for META:\n",
      "0   2012-12-12\n",
      "1   2012-12-13\n",
      "2   2012-12-14\n",
      "3   2012-12-15\n",
      "4   2012-12-16\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "META has 0 invalid dates (NaT) after conversion.\n",
      "META shape after dropping NaT: (4249, 9)\n",
      "\n",
      "Sample data for META after resampling:\n",
      "        Date   Open       High        Low      Close  Adj Close    Volume  \\\n",
      "0 2012-12-12  28.00  28.139999  27.370001  27.580000  27.523441  46704200   \n",
      "1 2012-12-13  27.59  28.750000  27.430000  28.240000  28.182087  81051600   \n",
      "2 2012-12-14  28.18  28.330000  26.760000  26.809999  26.755020  91631600   \n",
      "3 2012-12-15  28.18  28.330000  26.760000  26.809999  26.755020  91631600   \n",
      "4 2012-12-16  28.18  28.330000  26.760000  26.809999  26.755020  91631600   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in MSFT before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for MSFT:\n",
      "0   1986-03-13\n",
      "1   1986-03-14\n",
      "2   1986-03-15\n",
      "3   1986-03-16\n",
      "4   1986-03-17\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for MSFT:\n",
      "0   1986-03-13\n",
      "1   1986-03-14\n",
      "2   1986-03-15\n",
      "3   1986-03-16\n",
      "4   1986-03-17\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "MSFT has 0 invalid dates (NaT) after conversion.\n",
      "MSFT shape after dropping NaT: (14020, 9)\n",
      "\n",
      "Sample data for MSFT after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close      Volume  \\\n",
      "0 1986-03-13  0.088542  0.101563  0.088542  0.097222   0.059946  1031788800   \n",
      "1 1986-03-14  0.097222  0.102431  0.097222  0.100694   0.062087   308160000   \n",
      "2 1986-03-15  0.097222  0.102431  0.097222  0.100694   0.062087   308160000   \n",
      "3 1986-03-16  0.097222  0.102431  0.097222  0.100694   0.062087   308160000   \n",
      "4 1986-03-17  0.100694  0.103299  0.100694  0.102431   0.063158   133171200   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in NVDA before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for NVDA:\n",
      "0   1999-01-22\n",
      "1   1999-01-23\n",
      "2   1999-01-24\n",
      "3   1999-01-25\n",
      "4   1999-01-26\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for NVDA:\n",
      "0   1999-01-22\n",
      "1   1999-01-23\n",
      "2   1999-01-24\n",
      "3   1999-01-25\n",
      "4   1999-01-26\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "NVDA has 0 invalid dates (NaT) after conversion.\n",
      "NVDA shape after dropping NaT: (9322, 9)\n",
      "\n",
      "Sample data for NVDA after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close      Volume  \\\n",
      "0 1999-01-22  0.043750  0.048828  0.038802  0.041016   0.037621  2714688000   \n",
      "1 1999-01-23  0.043750  0.048828  0.038802  0.041016   0.037621  2714688000   \n",
      "2 1999-01-24  0.043750  0.048828  0.038802  0.041016   0.037621  2714688000   \n",
      "3 1999-01-25  0.044271  0.045833  0.041016  0.045313   0.041562   510480000   \n",
      "4 1999-01-26  0.045833  0.046745  0.041146  0.041797   0.038337   343200000   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n",
      "Columns in TSLA before conversion: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "\n",
      "Sample 'Date' values before conversion for TSLA:\n",
      "0   2010-06-29\n",
      "1   2010-06-30\n",
      "2   2010-07-01\n",
      "3   2010-07-02\n",
      "4   2010-07-03\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "Sample 'Date' values after conversion for TSLA:\n",
      "0   2010-06-29\n",
      "1   2010-06-30\n",
      "2   2010-07-01\n",
      "3   2010-07-02\n",
      "4   2010-07-03\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "TSLA has 0 invalid dates (NaT) after conversion.\n",
      "TSLA shape after dropping NaT: (5146, 9)\n",
      "\n",
      "Sample data for TSLA after resampling:\n",
      "        Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
      "0 2010-06-29  1.266667  1.666667  1.169333  1.592667   1.592667  281494500   \n",
      "1 2010-06-30  1.719333  2.028000  1.553333  1.588667   1.588667  257806500   \n",
      "2 2010-07-01  1.666667  1.728000  1.351333  1.464000   1.464000  123282000   \n",
      "3 2010-07-02  1.533333  1.540000  1.247333  1.280000   1.280000   77097000   \n",
      "4 2010-07-03  1.533333  1.540000  1.247333  1.280000   1.280000   77097000   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    for ticker in stock_data:\n",
    "        print(f\"Columns in {ticker} before conversion: {stock_data[ticker].columns.tolist()}\")\n",
    "        \n",
    "        print(f\"\\nSample 'Date' values before conversion for {ticker}:\")\n",
    "        print(stock_data[ticker]['Date'].head())\n",
    "        \n",
    "        stock_data[ticker]['Date'] = pd.to_datetime(stock_data[ticker]['Date'], errors='coerce')\n",
    "        \n",
    "        print(f\"\\nSample 'Date' values after conversion for {ticker}:\")\n",
    "        print(stock_data[ticker]['Date'].head())\n",
    "        \n",
    "        num_invalid_dates = stock_data[ticker]['Date'].isnull().sum()\n",
    "        print(f\"{ticker} has {num_invalid_dates} invalid dates (NaT) after conversion.\")\n",
    "        \n",
    "        stock_data[ticker] = stock_data[ticker].dropna(subset=['Date'])\n",
    "        print(f\"{ticker} shape after dropping NaT: {stock_data[ticker].shape}\")\n",
    "        \n",
    "        stock_data[ticker] = stock_data[ticker].set_index('Date').resample('D').ffill().reset_index()\n",
    "        print(f\"\\nSample data for {ticker} after resampling:\")\n",
    "        print(stock_data[ticker].head())\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error converting date columns: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9c2b8",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Define a function to analyze the sentiment of news headlines and apply it to create a new column with scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3360c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(headline):\n",
    "    \"\"\"Analyze sentiment of a headline and return the polarity score.\"\"\"\n",
    "    return TextBlob(headline).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to news headlines\n",
    "try:\n",
    "    news_df['sentiment'] = news_df['headline'].apply(analyze_sentiment)\n",
    "except Exception as e:\n",
    "    print(f\"Error during sentiment analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98704b",
   "metadata": {},
   "source": [
    "### Calculate Daily Stock Returns\n",
    "\n",
    "Compute the daily percentage change in stock prices to represent movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3d83cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for ticker in stock_data:\n",
    "        stock_data[ticker]['daily_return'] = stock_data[ticker]['Close'].pct_change()  # Calculate daily returns\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating daily returns: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bf91a",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Aggregate daily sentiment scores by averaging them and merge sentiment data with stock data to calculate the Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da62dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "news_df.rename(columns={'date': 'Date'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98edf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stock_data:\n",
    "    stock_data[ticker]['Date'] = pd.to_datetime(stock_data[ticker]['Date'], errors='coerce')\n",
    "    stock_data[ticker] = stock_data[ticker].dropna(subset=['Date'])\n",
    "    stock_data[ticker] = stock_data[ticker].set_index('Date').resample('D').ffill().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10ccdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = news_df.groupby('Date')['sentiment'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08ef782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For news_df / daily_sentiment\n",
    "daily_sentiment['Date'] = daily_sentiment['Date'].dt.tz_localize(None)\n",
    "\n",
    "# For stock_data (example for AAPL)\n",
    "stock_data['AAPL']['Date'] = stock_data['AAPL']['Date'].dt.tz_localize(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abf7d594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient for AAPL: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\metya\\anaconda3\\envs\\financial_analysis\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3037: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\Users\\metya\\anaconda3\\envs\\financial_analysis\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2894: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Users\\metya\\anaconda3\\envs\\financial_analysis\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2894: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    combined_df = pd.merge(stock_data['AAPL'], daily_sentiment, on='Date', how='inner')\n",
    "    correlation = combined_df['sentiment'].corr(combined_df['daily_return'])\n",
    "    print(f\"Pearson correlation coefficient for AAPL: {correlation}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating correlation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
